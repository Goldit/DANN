{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from random import shuffle\n",
    "from flip_gradient import flip_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_path       = \"toppad/\"\n",
    "source_file        = 'source.txt'\n",
    "target_file        = 'target.txt'\n",
    "test_file          = 'test.txt'\n",
    "val_file           = 'val.txt'\n",
    "\n",
    "test_set_size      = 100\n",
    "\n",
    "NUM_CHANNELS       = 3\n",
    "BATCH_SIZE         = 32\n",
    "IMAGE_SIZE         = 256\n",
    "NUM_CLASSES        = 2\n",
    "NUM_DOMAINES       = 2\n",
    "IMG_SIZE_CROPPED   = 224\n",
    "LEARNING_RATE      = 0.001\n",
    "DATASET_SIZE       = 40000\n",
    "STEPS_PER_EPOCH    = int(DATASET_SIZE/BATCH_SIZE)\n",
    "OPTIMIZER          = 'Adam'\n",
    "MODEL_NAME         = 'dann'\n",
    "LAMBDA             = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "  return int(label)\n",
    "\n",
    "def encode_size(size):\n",
    "    w,h = size.split(\"x\")\n",
    "    return (int(w), int(h))\n",
    "\n",
    "def read_image_list(file):\n",
    "  f = open(file, \"r\")\n",
    "  filepaths = []\n",
    "  labels = []\n",
    "  for line in f:\n",
    "    filepath, label, size = line.split(\" \")\n",
    "    filepaths.append(dataset_path + filepath)\n",
    "    labels.append(encode_label(label))\n",
    "  return filepaths, labels\n",
    "\n",
    "def read_images(input_queue):\n",
    "    file_content = tf.read_file(input_queue[0])\n",
    "    image = tf.image.decode_jpeg(file_content, channels=NUM_CHANNELS)\n",
    "    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS])\n",
    "    label = input_queue[1]  \n",
    "    return image, label\n",
    "\n",
    "def pre_process_image(image, training):\n",
    "    # This function takes a single image as input,\n",
    "    # and a boolean whether to build the training or testing graph.\n",
    "    # vgg mean\n",
    "    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 3], name='img_mean')\n",
    "    image = image - mean\n",
    "    if training:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Randomly crop the input image.\n",
    "        image = tf.random_crop(image, size=[IMG_SIZE_CROPPED, IMG_SIZE_CROPPED, NUM_CHANNELS])\n",
    "\n",
    "        # Randomly flip the image horizontally.\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Randomly adjust hue, contrast and saturation.\n",
    "        image = tf.image.random_hue(image, max_delta=0.05)\n",
    "        image = tf.image.random_contrast(image, lower=0.3, upper=1.0)\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        image = tf.image.random_saturation(image, lower=0.0, upper=2.0)\n",
    "\n",
    "        # Some of these functions may overflow and result in pixel\n",
    "        # values beyond the [0, 1] range. It is unclear from the\n",
    "        # documentation of TensorFlow 0.10.0rc0 whether this is\n",
    "        # intended. A simple solution is to limit the range.\n",
    "\n",
    "        # Limit the image pixels between [0, 1] in case of overflow.\n",
    "        # image = tf.minimum(image, 1.0)\n",
    "        # image = tf.maximum(image, 0.0)\n",
    "    else:\n",
    "        # For training, add the following to the TensorFlow graph.\n",
    "\n",
    "        # Crop the input image around the centre so it is the same\n",
    "        # size as images that are randomly cropped during training.\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image,\n",
    "                                                       target_height=IMG_SIZE_CROPPED,\n",
    "                                                       target_width=IMG_SIZE_CROPPED)\n",
    "\n",
    "    return image\n",
    "\n",
    "source_images, source_labels = read_image_list(dataset_path + source_file)\n",
    "target_images, target_labels = read_image_list(dataset_path + target_file)\n",
    "test_images, test_labels = read_image_list(dataset_path + test_file)\n",
    "val_images, val_labels = read_image_list(dataset_path + val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_vgg_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"images\"], [-1, IMG_SIZE_CROPPED, IMG_SIZE_CROPPED, NUM_CHANNELS], name=\"input_layer\")\n",
    "\n",
    "    # Convolutional Layer #1_1\n",
    "    conv1_1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv1_1\")\n",
    "    # Convolutional Layer #1_2\n",
    "    conv1_2 = tf.layers.conv2d(\n",
    "      inputs=conv1_1,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv1_2\")\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1_2, pool_size=[2, 2], strides=2, name=\"pool1\")\n",
    "\n",
    "    # Convolutional Layer #2_1\n",
    "    conv2_1 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv2_1\")\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2_2 = tf.layers.conv2d(\n",
    "      inputs=conv2_1,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv2_2\")\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2_2, pool_size=[2, 2], strides=2, name=\"pool2\")\n",
    "\n",
    "    # Convolutional Layer #3_1\n",
    "    conv3_1 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=256,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv3_1\")\n",
    "\n",
    "    # Convolutional Layer #3_1\n",
    "    conv3_2 = tf.layers.conv2d(\n",
    "      inputs=conv3_1,\n",
    "      filters=256,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv3_2\")\n",
    "\n",
    "    # Convolutional Layer #3_1\n",
    "    conv3_3 = tf.layers.conv2d(\n",
    "      inputs=conv3_2,\n",
    "      filters=256,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv3_3\")\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3_3, pool_size=[2, 2], strides=2, name=\"pool3\")    \n",
    "\n",
    "    # Convolutional Layer #4_1\n",
    "    conv4_1 = tf.layers.conv2d(\n",
    "      inputs=pool3,\n",
    "      filters=512,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv4_1\")\n",
    "\n",
    "    # Convolutional Layer #4_2\n",
    "    conv4_2 = tf.layers.conv2d(\n",
    "      inputs=conv4_1,\n",
    "      filters=512,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv4_2\")\n",
    "\n",
    "    # Convolutional Layer #4_3\n",
    "    conv4_3 = tf.layers.conv2d(\n",
    "      inputs=conv4_2,\n",
    "      filters=512,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv4_3\")\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    pool4 = tf.layers.max_pooling2d(inputs=conv4_3, pool_size=[2, 2], strides=2, name=\"pool4\")\n",
    "\n",
    "    # Convolutional Layer #5_1\n",
    "    conv5_1 = tf.layers.conv2d(\n",
    "      inputs=pool4,\n",
    "      filters=512,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv5_1\")\n",
    "\n",
    "    # Convolutional Layer #5_2\n",
    "    conv5_2 = tf.layers.conv2d(\n",
    "      inputs=conv5_1,\n",
    "      filters=512,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv5_2\")\n",
    "\n",
    "    # Convolutional Layer #5_3\n",
    "    conv5_3 = tf.layers.conv2d(\n",
    "      inputs=conv5_2,\n",
    "      filters=512,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu,\n",
    "      name=\"conv5_3\")\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    pool5 = tf.layers.max_pooling2d(inputs=conv5_3, pool_size=[2, 2], strides=2, name=\"pool5\")\n",
    "\n",
    "    # FC Layers\n",
    "    all_features =tf.contrib.layers.flatten(pool5)\n",
    "    with tf.name_scope('extract_lb_feat'):\n",
    "        source_features =  tf.strided_slice(all_features, [0], [tf.to_int32(all_features.shape[0])], [2])\n",
    "        classify_feats = tf.cond(tf.cast(mode == learn.ModeKeys.TRAIN, tf.bool), lambda:source_features, lambda:all_features)\n",
    "    lb_fc1   = tf.layers.dense(inputs=classify_feats, units=4096, activation=tf.nn.relu, name=\"lb_fc1\")\n",
    "    lb_dropout1 = tf.layers.dropout(inputs=lb_fc1, rate=0.5, training=mode==learn.ModeKeys.TRAIN, name=\"lb_dropout1\")\n",
    "    lb_fc2   = tf.layers.dense(inputs=lb_dropout1, units=4096, activation=tf.nn.relu, name=\"lb_fc2\")\n",
    "    lb_dropout2 = tf.layers.dropout(inputs=lb_fc2, rate=0.5, training=mode==learn.ModeKeys.TRAIN, name=\"lb_dropout2\")\n",
    "    label_logits = tf.layers.dense(inputs=lb_dropout2, units=NUM_CLASSES, name=\"lb_softmax\")\n",
    "\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        with tf.name_scope('flip_gradient'):\n",
    "            flipped_features = flip_gradient(all_features)\n",
    "        dm_fc1   = tf.layers.dense(inputs=flipped_features, units=4096, activation=tf.nn.relu, name=\"dm_fc1\")\n",
    "        dm_dropout1 = tf.layers.dropout(inputs=dm_fc1, rate=0.5, training=mode==learn.ModeKeys.TRAIN, name=\"dm_dropout1\")\n",
    "        dm_fc2   = tf.layers.dense(inputs=dm_dropout1, units=4096, activation=tf.nn.relu, name=\"dm_fc2\")\n",
    "        dm_dropout2 = tf.layers.dropout(inputs=dm_fc2, rate=0.5, training=mode==learn.ModeKeys.TRAIN, name=\"dm_dropout2\")\n",
    "        domain_logits = tf.layers.dense(inputs=dm_dropout2, units=NUM_DOMAINES, name=\"dm_softmax\")\n",
    "\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    \n",
    "    if mode != learn.ModeKeys.INFER:\n",
    "        with tf.name_scope('label_loss'):\n",
    "            source_labels = tf.strided_slice(labels, [0], [tf.to_int32(labels.shape[0])], [2])\n",
    "            classify_labels = tf.cond(tf.cast(mode == learn.ModeKeys.TRAIN, tf.bool), lambda:source_labels, lambda:labels)\n",
    "            onehot_labels = tf.one_hot(indices=tf.cast(classify_labels, tf.int32), depth=NUM_CLASSES)\n",
    "            label_loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=label_logits)\n",
    "            loss = label_loss\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == learn.ModeKeys.TRAIN:\n",
    "        with tf.name_scope('domain_loss'):\n",
    "            domain_labels = tf.constant([i%2 for i in range(BATCH_SIZE)], tf.int32)\n",
    "            onehot_domain_labels = tf.one_hot(indices=tf.cast(domain_labels, tf.int32), depth=NUM_DOMAINES)\n",
    "            domain_loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_domain_labels, logits=domain_logits)\n",
    "        with tf.name_scope('total_loss'):\n",
    "            loss = label_loss + domain_loss\n",
    "            tf.summary.scalar(\"label_losss\", label_loss)\n",
    "            tf.summary.scalar(\"domain_loss\", domain_loss)\n",
    "        train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=label_loss + domain_loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            optimizer=OPTIMIZER)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        # Generate Predictions\n",
    "        predictions = {\n",
    "          \"classes\": tf.argmax(\n",
    "              input=label_logits, axis=1),\n",
    "          \"probabilities\": tf.nn.softmax(\n",
    "              label_logits, name=\"softmax_tensor\")\n",
    "        } \n",
    "\n",
    "    # Return a ModelFnOps object\n",
    "    return model_fn_lib.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_input_fn(images, labels, batch_size=32, training=True, num_epochs=1):  \n",
    "    with tf.name_scope('input_batch'):\n",
    "        # create input queues\n",
    "        input_queue = tf.train.slice_input_producer(\n",
    "                            [images, labels],\n",
    "                            num_epochs=num_epochs,\n",
    "                            shuffle=True)\n",
    "\n",
    "        image, label = read_images(input_queue)\n",
    "\n",
    "        # preprocess image\n",
    "        image = pre_process_image(tf.to_float(image), training)\n",
    "\n",
    "        # create batch\n",
    "        batch_dict = tf.train.batch(dict(images=image, labels=label) , batch_size,\n",
    "                                    num_threads=1, capacity=batch_size*2, \n",
    "                                    enqueue_many=False, shapes=None, dynamic_pad=False, \n",
    "                                    allow_smaller_final_batch=False, \n",
    "                                    shared_name=None, name=None)\n",
    "\n",
    "        batch_labels = batch_dict.pop('labels')\n",
    "        return batch_dict, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def batch_input_train_fn(source_images, source_labels, target_images, target_labels, batch_size=64, training=True, num_epochs=1):  \n",
    "    with tf.name_scope('input_batch'):\n",
    "        # create input queues\n",
    "        input_source_queue = tf.train.slice_input_producer(\n",
    "                            [source_images, source_labels],\n",
    "                            num_epochs=num_epochs,\n",
    "                            shuffle=True,\n",
    "                            name='synthetic_image')\n",
    "\n",
    "        input_target_queue = tf.train.slice_input_producer(\n",
    "                            [target_images, target_labels],\n",
    "                            num_epochs=num_epochs,\n",
    "                            shuffle=True,\n",
    "                            name='real_image')\n",
    "\n",
    "        source_image, source_label = read_images(input_source_queue)\n",
    "        target_image, target_label = read_images(input_target_queue)\n",
    "\n",
    "        # preprocess image\n",
    "        source_image = pre_process_image(tf.to_float(source_image), training)\n",
    "        target_image = pre_process_image(tf.to_float(target_image), training)\n",
    "        # create batch\n",
    "        batch_dict = tf.train.batch(dict(images=[source_image, target_image], labels=[source_label, target_label]) , batch_size,\n",
    "                                    num_threads=1, capacity=batch_size*2, \n",
    "                                    enqueue_many=True, shapes=None, dynamic_pad=False, \n",
    "                                    allow_smaller_final_batch=False, \n",
    "                                    shared_name=None, name=None)\n",
    "    \n",
    "    batch_labels = batch_dict.pop('labels')\n",
    "    return batch_dict, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_save_checkpoints_secs': 600, '_save_summary_steps': 100, '_evaluation_master': '', '_task_id': 0, '_model_dir': None, '_num_worker_replicas': 0, '_keep_checkpoint_max': 5, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_environment': 'local', '_tf_random_seed': None, '_save_checkpoints_steps': None, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbea2185f98>, '_keep_checkpoint_every_n_hours': 10000, '_num_ps_replicas': 0, '_is_chief': True}\n"
     ]
    }
   ],
   "source": [
    "# Create the Estimator\n",
    "toppad_classifier = learn.Estimator(model_fn=cnn_model_vgg_fn, model_dir=\"toppad_classifier/{}_vgg16_{}_lr{}_lb{}\".format(MODEL_NAME, OPTIMIZER, LEARNING_RATE, LAMBDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n"
     ]
    }
   ],
   "source": [
    "# Configure validation and test hooks\n",
    "toppad_validator = learn.monitors.ValidationMonitor(\n",
    "      input_fn=lambda: batch_input_fn(val_images, val_labels, batch_size=BATCH_SIZE, training=False),\n",
    "      every_n_steps=STEPS_PER_EPOCH,\n",
    "      metrics={\"accuracy_synthetic\": learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),},)\n",
    "\n",
    "toppad_tester = learn.monitors.ValidationMonitor(\n",
    "      input_fn=lambda: batch_input_fn(test_images, test_labels, batch_size=BATCH_SIZE, training=False),\n",
    "      every_n_steps=STEPS_PER_EPOCH,\n",
    "      metrics={\"accuracy_real\": learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key=\"classes\"),},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-1250\n",
      "INFO:tensorflow:Saving checkpoints for 1251 into toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.38776, step = 1251\n",
      "INFO:tensorflow:global_step/sec: 2.11687\n",
      "INFO:tensorflow:loss = 1.37929, step = 1351 (47.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11796\n",
      "INFO:tensorflow:loss = 1.3916, step = 1451 (47.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11839\n",
      "INFO:tensorflow:loss = 1.37754, step = 1551 (47.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1331\n",
      "INFO:tensorflow:loss = 1.38577, step = 1651 (46.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12411\n",
      "INFO:tensorflow:loss = 1.40581, step = 1751 (47.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12064\n",
      "INFO:tensorflow:loss = 1.39025, step = 1851 (47.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11731\n",
      "INFO:tensorflow:loss = 1.38371, step = 1951 (47.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11252\n",
      "INFO:tensorflow:loss = 1.39295, step = 2051 (47.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11315\n",
      "INFO:tensorflow:loss = 1.39015, step = 2151 (47.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11113\n",
      "INFO:tensorflow:loss = 1.37037, step = 2251 (47.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.10138\n",
      "INFO:tensorflow:loss = 1.37251, step = 2351 (47.587 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11195\n",
      "INFO:tensorflow:loss = 1.38685, step = 2451 (47.350 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:12:48\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-1251\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:13:07\n",
      "INFO:tensorflow:Saving dict for global step 1251: accuracy_synthetic = 0.50475, global_step = 1251, loss = 0.692934\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 2500): global_step = 1251, loss = 0.692934, accuracy_synthetic = 0.50475\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:13:08\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-1251\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:13:10\n",
      "INFO:tensorflow:Saving dict for global step 1251: accuracy_real = 0.497396, global_step = 1251, loss = 0.6915\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 2500): global_step = 1251, loss = 0.6915, accuracy_real = 0.497396\n",
      "INFO:tensorflow:Saving checkpoints for 2501 into toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.34966\n",
      "INFO:tensorflow:loss = 1.38582, step = 2551 (74.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11166\n",
      "INFO:tensorflow:loss = 1.38576, step = 2651 (47.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11313\n",
      "INFO:tensorflow:loss = 1.39464, step = 2751 (47.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11867\n",
      "INFO:tensorflow:loss = 1.38977, step = 2851 (47.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13441\n",
      "INFO:tensorflow:loss = 1.39346, step = 2951 (46.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13836\n",
      "INFO:tensorflow:loss = 1.38252, step = 3051 (46.765 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11897\n",
      "INFO:tensorflow:loss = 1.38822, step = 3151 (47.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.089\n",
      "INFO:tensorflow:loss = 1.3778, step = 3251 (47.871 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.01357\n",
      "INFO:tensorflow:loss = 1.3828, step = 3351 (49.663 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13189\n",
      "INFO:tensorflow:loss = 1.40488, step = 3451 (46.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12084\n",
      "INFO:tensorflow:loss = 1.40705, step = 3551 (47.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1245\n",
      "INFO:tensorflow:loss = 1.38352, step = 3651 (47.069 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:23:07\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-2501\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:23:26\n",
      "INFO:tensorflow:Saving dict for global step 2501: accuracy_synthetic = 0.4955, global_step = 2501, loss = 0.693345\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 3750): global_step = 2501, loss = 0.693345, accuracy_synthetic = 0.4955\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:23:27\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-2501\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:23:29\n",
      "INFO:tensorflow:Saving dict for global step 2501: accuracy_real = 0.5, global_step = 2501, loss = 26121.2\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 3750): global_step = 2501, loss = 26121.2, accuracy_real = 0.5\n",
      "INFO:tensorflow:Saving checkpoints for 3751 into toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.36187\n",
      "INFO:tensorflow:loss = 1.38488, step = 3751 (73.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.98186\n",
      "INFO:tensorflow:loss = 1.37889, step = 3851 (50.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99848\n",
      "INFO:tensorflow:loss = 1.39212, step = 3951 (50.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11448\n",
      "INFO:tensorflow:loss = 1.37914, step = 4051 (47.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.1051\n",
      "INFO:tensorflow:loss = 1.39397, step = 4151 (47.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11068\n",
      "INFO:tensorflow:loss = 1.39819, step = 4251 (47.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11031\n",
      "INFO:tensorflow:loss = 1.37744, step = 4351 (47.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11848\n",
      "INFO:tensorflow:loss = 1.396, step = 4451 (47.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11229\n",
      "INFO:tensorflow:loss = 1.39037, step = 4551 (47.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12567\n",
      "INFO:tensorflow:loss = 1.38056, step = 4651 (47.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11531\n",
      "INFO:tensorflow:loss = 1.38507, step = 4751 (47.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13128\n",
      "INFO:tensorflow:loss = 1.40005, step = 4851 (46.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.14548\n",
      "INFO:tensorflow:loss = 1.38865, step = 4951 (46.610 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:33:29\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-3751\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:33:52\n",
      "INFO:tensorflow:Saving dict for global step 3751: accuracy_synthetic = 0.49525, global_step = 3751, loss = 0.693905\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 5000): global_step = 3751, loss = 0.693905, accuracy_synthetic = 0.49525\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:33:53\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-3751\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:33:57\n",
      "INFO:tensorflow:Saving dict for global step 3751: accuracy_real = 0.494792, global_step = 3751, loss = 0.693933\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 5000): global_step = 3751, loss = 0.693933, accuracy_real = 0.494792\n",
      "INFO:tensorflow:Saving checkpoints for 5001 into toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.23912\n",
      "INFO:tensorflow:loss = 1.38318, step = 5051 (80.707 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99793\n",
      "INFO:tensorflow:loss = 1.40001, step = 5151 (50.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0035\n",
      "INFO:tensorflow:loss = 1.39543, step = 5251 (49.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 1.99909\n",
      "INFO:tensorflow:loss = 1.4142, step = 5351 (50.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0955\n",
      "INFO:tensorflow:loss = 1.3989, step = 5451 (47.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.11378\n",
      "INFO:tensorflow:loss = 1.41101, step = 5551 (47.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.08322\n",
      "INFO:tensorflow:loss = 1.38401, step = 5651 (48.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.00193\n",
      "INFO:tensorflow:loss = 1.39031, step = 5751 (49.952 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.0204\n",
      "INFO:tensorflow:loss = 1.40184, step = 5851 (49.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.07153\n",
      "INFO:tensorflow:loss = 1.39189, step = 5951 (48.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.09824\n",
      "INFO:tensorflow:loss = 1.38697, step = 6051 (47.659 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12152\n",
      "INFO:tensorflow:loss = 1.38421, step = 6151 (47.136 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6227 into toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:44:13\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-6227\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:44:37\n",
      "INFO:tensorflow:Saving dict for global step 6227: accuracy_synthetic = 0.50475, global_step = 6227, loss = 0.693105\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 6250): global_step = 6227, loss = 0.693105, accuracy_synthetic = 0.50475\n",
      "INFO:tensorflow:Starting evaluation at 2017-04-20-07:44:38\n",
      "INFO:tensorflow:Restoring parameters from toppad_classifier/dann_vgg16_Adam_0.001/model.ckpt-6227\n",
      "INFO:tensorflow:Finished evaluation at 2017-04-20-07:44:41\n",
      "INFO:tensorflow:Saving dict for global step 6227: accuracy_real = 0.492188, global_step = 6227, loss = 0.693401\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 6250): global_step = 6227, loss = 0.693401, accuracy_real = 0.492188\n",
      "INFO:tensorflow:global_step/sec: 1.23356\n",
      "INFO:tensorflow:loss = 1.40644, step = 6251 (81.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13195\n",
      "INFO:tensorflow:loss = 1.39701, step = 6351 (46.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.13294\n",
      "INFO:tensorflow:loss = 1.39951, step = 6451 (46.883 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12593\n",
      "INFO:tensorflow:loss = 1.3747, step = 6551 (47.038 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12052\n",
      "INFO:tensorflow:loss = 1.38223, step = 6651 (47.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12961\n",
      "INFO:tensorflow:loss = 1.39019, step = 6751 (46.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.119\n",
      "INFO:tensorflow:loss = 1.38385, step = 6851 (47.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 2.12864\n",
      "INFO:tensorflow:loss = 1.39238, step = 6951 (46.978 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bd0add38e615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   input_fn=lambda: batch_input_train_fn(source_images, source_labels, target_images, target_labels, \n\u001b[1;32m      5\u001b[0m   batch_size=BATCH_SIZE, training=True, num_epochs=num_epochs),\n\u001b[0;32m----> 6\u001b[0;31m   monitors=[toppad_validator, toppad_tester])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             func.__module__, arg_name, date, instructions)\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\n\u001b[1;32m    283\u001b[0m         func.__doc__, date, instructions)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_fn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m       \u001b[0msummary_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSummaryWriterCache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    482\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    818\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    821\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    928\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs  = 999\n",
    "toppad_classifier.fit(\n",
    "  input_fn=lambda: batch_input_train_fn(source_images, source_labels, target_images, target_labels, \n",
    "  batch_size=BATCH_SIZE, training=True, num_epochs=num_epochs),\n",
    "  monitors=[toppad_validator, toppad_tester])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
